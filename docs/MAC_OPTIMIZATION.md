# Apple Silicon Optimization Guide - PyfaceAU

**Hardware acceleration for ARM MacBook (M1/M2/M3) - Accuracy-Preserving Only**

---

## Overview

Current performance on Apple Silicon:
- **Sequential (CPU only)**: 4.6 FPS (217ms/frame)
- **Parallel (6 workers)**: ~28 FPS (36ms/frame)

**Target with Apple-specific optimizations: 50-100 FPS** âœ¨

**Important:** All optimizations preserve accuracy - identical results to C++ OpenFace.

---

## What Works on Apple Silicon

| Technology | Works? | Performance | Accuracy | Effort |
|------------|--------|-------------|----------|--------|
| **CoreML (Neural Engine)** | YES | 2-5x | 100% | Easy |
| **Metal Performance Shaders** | YES | 3-10x | 100% | Medium |
| **Accelerate Framework** | YES | 2-4x | 100% | Medium |
| **PyTorch MPS** | YES | 2-5x | 100% | Medium |
| **Parallel Processing** | YES | 6x | 100% | Easy |
| CUDA/CuPy | NO | N/A | N/A | N/A |
| Numba | Warning: LIMITED | 1.2-2x | 100% | Easy |

---

## Component-Specific Optimizations

### 1. Neural Network Models (ONNX) - CoreML Acceleration

**Components:** Face Detection (RetinaFace), Landmark Detection (PFLD)

**Current:** RetinaFace + PFLD = ~32ms/frame (tracked)
**Target:** ~8-15ms/frame (2-4x speedup)

#### Enable CoreML Neural Engine

CoreML leverages Apple's Neural Engine for hardware-accelerated inference:

```python
from pyfaceau.pipeline import FullPythonAUPipeline

# Enable CoreML for neural network acceleration
pipeline = FullPythonAUPipeline(
    retinaface_model='weights/retinaface_mobilenet025_coreml.onnx',
    pfld_model='weights/pfld_cunjian.onnx',
    pdm_file='weights/In-the-wild_aligned_PDM_68.txt',
    au_models_dir='weights/AU_predictors',
    triangulation_file='weights/tris_68_full.txt',
    use_coreml=True,  # Enable Neural Engine
    track_faces=True,
    verbose=True
)
```

**Already Implemented!** 

**Performance:**
- RetinaFace: ~150ms â†’ ~50ms (3x faster)
- PFLD: ~30ms â†’ ~10ms (3x faster)

**Accuracy:** 100% identical to ONNX CPU

---

### 2. CalcParams (Pose Estimation) - HIGHEST PRIORITY

**Current:** 80ms/frame (37% of total time)
**Target:** 20-40ms/frame (2-4x speedup)

CalcParams is pure Python matrix operations - perfect for Apple Accelerate framework.

#### Option A: Use Apple Accelerate BLAS (BEST FOR MAC)

NumPy on macOS automatically uses Accelerate framework if installed correctly:

```bash
# Check if NumPy is using Accelerate
python -c "import numpy as np; np.__config__.show()"

# Should show:
# BLAS: Accelerate
# LAPACK: Accelerate
```

If not using Accelerate, reinstall NumPy:

```bash
# Uninstall current NumPy
pip uninstall numpy

# Reinstall with Accelerate support (default on macOS)
pip install numpy

# Verify
python -c "import numpy as np; np.__config__.show()"
```

**Expected speedup:** 1.5-2x (80ms â†’ 40-53ms)

**Accuracy:** 100% identical (same algorithms, just faster BLAS)

---

#### Option B: Numba JIT Compilation (LIMITED ON ARM)

Numba has limited ARM support but can provide moderate speedup:

```python
from numba import jit
import numpy as np

@jit(nopython=True, cache=True, fastmath=True)
def compute_jacobian(params, mean_shape, basis_vectors, landmarks):
    """JIT-compiled Jacobian computation"""
    # ... matrix operations ...
    return J, residuals

class CalcParams:
    def calc_params(self, landmarks):
        # Use JIT-compiled functions for hot loops
        for iteration in range(max_iters):
            J, residuals = compute_jacobian(params, self.mean_shape,
                                           self.basis_vectors, landmarks)
            # ... solve system ...
```

**Installation:**
```bash
pip install numba
```

**Expected speedup:** 1.2-2x (80ms â†’ 40-67ms)

**Accuracy:** 100% identical (same math)

**Note:** ARM support is experimental - test thoroughly!

---

#### Option C: PyTorch MPS Backend (ADVANCED)

PyTorch on Apple Silicon uses Metal Performance Shaders:

```python
import torch

class CalcParamsMPS:
    def __init__(self, pdm_parser):
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

        # Move matrices to MPS (Metal)
        self.mean_shape = torch.from_numpy(pdm_parser.mean_shape).to(self.device)
        self.basis_vectors = torch.from_numpy(pdm_parser.basis_vectors).to(self.device)

    def calc_params(self, landmarks):
        # Convert to PyTorch tensor
        landmarks_t = torch.from_numpy(landmarks).to(self.device)

        # Gauss-Newton iterations on Metal GPU
        for iteration in range(max_iters):
            # All matrix operations run on Metal
            J = self.compute_jacobian(params, landmarks_t)
            JtJ = J.T @ J
            Jtr = J.T @ residuals
            delta = torch.linalg.solve(JtJ, Jtr)
            params = params - delta

        # Convert back to NumPy
        return params.cpu().numpy()
```

**Installation:**
```bash
pip install torch torchvision
```

**Expected speedup:** 2-4x (80ms â†’ 20-40ms)

**Accuracy:** 100% identical (same math, different backend)

**Pros:** Significant speedup, leverages Metal GPU
**Cons:** Requires rewrite, memory transfer overhead

---

### 3. SVR Predictions (17 Models) - MEDIUM PRIORITY

**Current:** 30ms/frame (14% of total time)
**Target:** 6-12ms/frame (2-5x speedup)

#### Option A: Batch All Predictions (EASIEST, BEST ROI)

Stack all SVR models and compute in one matrix operation:

```python
class BatchedAUPredictor:
    def __init__(self, au_models):
        """Pre-stack all SVR model parameters"""
        # Stack all support vectors (17, 4702)
        self.all_support_vectors = np.vstack([
            model['support_vectors'] for model in au_models.values()
        ])

        # Stack all means (17, 4702)
        self.all_means = np.vstack([
            model['means'].flatten() for model in au_models.values()
        ])

        # Collect all biases (17,)
        self.all_biases = np.array([
            model['bias'] for model in au_models.values()
        ])

        # Dynamic model mask (17,)
        self.dynamic_mask = np.array([
            model['model_type'] == 'dynamic' for model in au_models.values()
        ])

    def predict_all(self, full_vector, running_median):
        """Predict all 17 AUs in one matrix operation"""
        # Broadcast full_vector to (17, 4702)
        centered = full_vector - self.all_means

        # Subtract running median for dynamic models only
        centered[self.dynamic_mask] -= running_median

        # Single matrix multiplication for all AUs
        # Uses Accelerate BLAS automatically on Mac
        predictions = np.sum(centered * self.all_support_vectors, axis=1) + self.all_biases

        # Clamp to [0, 5]
        predictions = np.clip(predictions, 0.0, 5.0)

        return predictions
```

**Expected speedup:** 2-5x (30ms â†’ 6-15ms)

**Accuracy:** 100% identical (same computation, just vectorized)

**Pros:**
- Easy to implement
- No dependencies
- Uses Accelerate BLAS automatically
- Preserves accuracy

**Implementation effort:** 2-3 hours

---

#### Option B: PyTorch MPS for Matrix Ops

```python
import torch

class MPSBatchedAUPredictor:
    def __init__(self, au_models):
        self.device = torch.device("mps")

        # Move all matrices to Metal GPU
        self.all_support_vectors = torch.from_numpy(np.vstack([...])).to(self.device)
        self.all_means = torch.from_numpy(np.vstack([...])).to(self.device)
        self.all_biases = torch.from_numpy(np.array([...])).to(self.device)
        self.dynamic_mask = torch.from_numpy(np.array([...])).to(self.device)

    def predict_all(self, full_vector, running_median):
        # Convert to PyTorch
        full_vector_t = torch.from_numpy(full_vector).to(self.device)
        running_median_t = torch.from_numpy(running_median).to(self.device)

        # All operations on Metal GPU
        centered = full_vector_t - self.all_means
        centered[self.dynamic_mask] -= running_median_t

        predictions = torch.sum(centered * self.all_support_vectors, dim=1) + self.all_biases
        predictions = torch.clip(predictions, 0.0, 5.0)

        # Return to CPU
        return predictions.cpu().numpy()
```

**Expected speedup:** 3-7x (30ms â†’ 4-10ms)

**Accuracy:** 100% identical

**Pros:** Maximum speedup for AU prediction
**Cons:** Memory transfer overhead

---

### 4. HOG Extraction - LOW PRIORITY (Already Optimized)

**Current:** 50ms/frame (23% of total time)

PyFHOG is already a C extension, so limited optimization available without changing results.

**No accuracy-preserving optimizations available for Mac.**

Warning: **Do NOT:**
- Increase cell_size (changes features)
- Reduce aligned face size (changes features)
- Use different HOG implementation (different features)

These would change AU accuracy vs C++ OpenFace.

---

### 5. Face Alignment - LOW PRIORITY

**Current:** 20ms/frame (9% of total time)

Already uses optimized NumPy operations with Accelerate BLAS.

**Minimal gains available without changing accuracy.**

---

## Recommended Implementation Plan

### Phase 1: Quick Wins (1 day, ~7x speedup)

**Already Done:**
1. CoreML acceleration (use_coreml=True)
2. Face tracking (track_faces=True)
3. Parallel processing (ParallelAUPipeline)

**Result:** 4.6 FPS â†’ 28 FPS (6x)

### Phase 2: Batched SVR (2-3 hours, +15% speedup)

**Action:** Implement batched AU predictions

```python
# In pipeline.py, replace _predict_aus with batched version
def _predict_aus(self, hog_features, geom_features, running_median):
    full_vector = np.concatenate([hog_features, geom_features])
    return self.batched_predictor.predict_all(full_vector, running_median)
```

**Result:** 28 FPS â†’ 32 FPS (+15%)

### Phase 3: Optimize CalcParams (1-2 days, +25% speedup)

**Option A (Easier):** Verify NumPy uses Accelerate BLAS

```bash
python -c "import numpy as np; np.__config__.show()"
# Should show: BLAS: Accelerate
```

**Option B (Better):** Implement PyTorch MPS version

**Result:** 32 FPS â†’ 40 FPS (+25%)

### Phase 4: Advanced (3-5 days, +40% speedup)

**Action:** Full PyTorch MPS CalcParams + AU prediction

**Result:** 40 FPS â†’ 56 FPS (+40%)

---

## Expected Performance Roadmap (MacBook)

| Stage | FPS | Speedup | Accuracy | Effort |
|-------|-----|---------|----------|--------|
| Baseline (CPU) | 4.6 | 1x | 100% | - |
| + CoreML + Tracking | 4.6 | 1x | 100% | Done |
| + Parallel (6 workers) | 28 | 6x | 100% | Done |
| + Batched SVR | 32 | 7x | 100% | ðŸŸ¢ 2-3 hours |
| + Accelerate BLAS | 36 | 8x | 100% | ðŸŸ¢ Verify only |
| + Numba CalcParams | 40 | 9x | 100% | ðŸŸ¡ 1 day |
| + PyTorch MPS (CalcParams) | 48 | 10x | 100% | ðŸŸ¡ 2-3 days |
| + PyTorch MPS (SVR) | **56** | **12x** | **100%** | ðŸ”´ 3-5 days |

**Realistic Target:** 40-56 FPS with 100% accuracy preservation

---

## Installation for MacBook

### Check Current Setup

```bash
# Check NumPy BLAS (should show Accelerate)
python -c "import numpy as np; np.__config__.show()"

# Check PyTorch MPS availability
python -c "import torch; print(torch.backends.mps.is_available())"

# Check CoreML availability
python -c "import coremltools; print('CoreML available')"
```

### Install Required Packages

```bash
# Basic (Numba for JIT)
pip install numba

# Advanced (PyTorch MPS)
pip install torch torchvision torchaudio

# CoreML tools (for model conversion)
pip install coremltools
```

---

## Testing on MacBook

### Run Detailed Benchmark

```bash
cd "S0 PyfaceAU"

# Test current performance
python benchmark_detailed.py \
    --video test_video.mp4 \
    --max-frames 200
```

This will identify your specific bottlenecks on Apple Silicon.

### Monitor Metal GPU Usage

```bash
# In another terminal, monitor GPU usage
sudo powermetrics --samplers gpu_power -i 1000

# Or use Activity Monitor â†’ Window â†’ GPU History
```

---

## What NOT to Do (Accuracy Loss)

**Do NOT:**
1. Reduce HOG cell_size (8 â†’ 12) - Changes features
2. Reduce aligned face size (112 â†’ 96) - Changes features
3. Reduce CalcParams iterations - Changes pose accuracy
4. Use approximate matrix solvers - Changes pose accuracy
5. Quantize models - Changes neural network outputs

**Safe Optimizations:**
1. Batched matrix operations (vectorization)
2. Better BLAS libraries (Accelerate)
3. JIT compilation (same math, faster execution)
4. Metal/MPS acceleration (same math, GPU backend)
5. Parallel processing (independent frames)
6. CoreML (exact same model, faster inference)

---

## Summary for MacBook Users

**Best options for Apple Silicon:**

1. **Parallel processing** - 6x speedup, already implemented 
2. **Batched SVR predictions** - 15% additional speedup, 2-3 hours
3. **Accelerate BLAS** - 10-15% additional speedup, just verify
4. **PyTorch MPS for CalcParams** - 25-40% additional speedup, 2-3 days

**Realistic target: 40-56 FPS with 100% accuracy preservation**

**Not worth it on Mac:**
- CUDA/CuPy (requires NVIDIA GPU)
- GPU HOG (no Metal implementation available)
- JAX (no Metal backend)

---

**Questions? Test on your MacBook and share results!**
